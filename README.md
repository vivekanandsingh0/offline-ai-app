# Cortex Runtime

Cortex is a **fully offline**, constitution-governed local AI runtime built with **React Native (Expo)**. It enables decentralized intelligence that runs directly on your device using a custom native C++ bridge to **llama.cpp**.

> **Status**: Active Development (Cortex-First Refactor)

## ğŸ›ï¸ The Cortex Constitution
Cortex is not just an app; it's a runtime governed by a foundational charter. It prioritizes local intelligence, user sovereignty, and ethical AI behavior. Every generation is validated against constitutional safety rules.

## ğŸ§© Modularity & Knowledge Packs
Cortex is domain-agnostic. All specialized knowledge (Syllabus, Subjects, Professional Data) is stored in external, optional **Knowledge Packs**.
- **Constitutional Mode**: Default state for general assistance.
- **Knowledge-Grounded Mode**: Triggered when a validated pack is discovered.

## ğŸŒŸ Key Features
- **Zero Cloud**: 100% offline inference and prompt processing.
- **Dynamic Knowledge**: Discovery of subject-based packs at runtime.
- **Validation Engine**: Post-generation safety and constitutional checks.
- **Minimalist Design**: A clean, distraction-free interface focused on intelligence.

### ğŸ’¬ Chat Experience
- **Persistent History**: Auto-saves conversations. View and restore past chats.
- **Context-Aware Prompts**: System prompts adapt based on the user's class (Nursery to Class 10).
- **Tool Support**: (UI Ready) Selector for tools like Calculator, Canvas, etc.
- **Response Caching**: Instant responses for previously asked questions to save battery and time.

### âš™ï¸ Model Management
- **Download Manager**: Native UI to download supported GGUF models directly to device storage.
- **Progress Tracking**: Real-time download progress bars.
- **Auto-Load**: Automatically loads the last used model on app startup.

### ğŸ¨ User Customization
- **Class-Based Personalization**: Tailors AI complexity and tone based on user's grade level.
- **Theming**: System, Light, and Dark mode support.
- **Language**: UI support for English, Hindi, Spanish, French.

## ğŸ› ï¸ Technical Stack

- **Framework**: React Native (Expo SDK 52)
- **Language**: TypeScript
- **State Management**: Zustand (Stores: `User`, `Chat`, `Model`)
- **Native Module**: Custom Android Module (`OfflineLLMModule`)
  - **Language**: Kotlin (Android) + C++ (JNI)
  - **Core Engine**: `llama.cpp` (GGUF inference)
- **Storage**: `expo-file-system` (Settings, Chat History, Model Files)
- **Navigation**: `expo-router`

## ğŸ“‚ Project Structure

```
offline-ai-app/
â”œâ”€â”€ app/                    # Expo Router pages
â”‚   â”œâ”€â”€ (tabs)/             # Main tab navigation (Chat, Host, Settings)
â”‚   â”œâ”€â”€ _layout.tsx         # Root layout & providers
â”‚   â””â”€â”€ index.tsx           # Entry & redirections
â”œâ”€â”€ components/             # Reusable UI components
â”‚   â”œâ”€â”€ ChatHistoryModal    # Sidebar for past chats
â”‚   â”œâ”€â”€ ClassSelectionModal # Onboarding/Settings modal
â”‚   â””â”€â”€ ...
â”œâ”€â”€ native/                 # Custom Native Modules
â”‚   â””â”€â”€ OfflineLLMModule/   # The bridge to llama.cpp
â”‚       â”œâ”€â”€ android/        # Kotlin & C++ source implementation
â”‚       â””â”€â”€ src/            # TS definitions
â”œâ”€â”€ store/                  # Zustand State Stores
â”‚   â”œâ”€â”€ useChatStore.ts     # Chat history persistence
â”‚   â”œâ”€â”€ useModelStore.ts    # Model download & state
â”‚   â””â”€â”€ useUserStore.ts     # User settings & preferences
â””â”€â”€ utils/                  # Helpers (PromptBuilder, ResponseCache)
```

## ğŸš€ Getting Started

### Prerequisites
- Node.js & npm/yarn
- Android Studio (for compiling the C++ native module)
- Android Device (Emulator support is limited for extensive local LLM inference)

### Installation

1.  **Clone the repository**:
    ```bash
    cd cortex-app
    ```

2.  **Install dependencies**:
    ```bash
    npm install
    ```

3.  **Run on Android**:
    ```bash
    npx expo run:android
    ```
    *Note: This will trigger the Gradle build process to compile the C++ `llama.cpp` libraries. The first build may take several minutes.*

## ğŸ§© Native Module Info

The app uses a custom JNI bridge to communicate with `llama.cpp`.
- **Java/Kotlin Layer**: Handles file downloads and exposes methods like `loadModel()`, `generate()`, `stopGeneration()`.
- **C++ Layer**: Directly calls `llama.cpp` functions to load GGUF files and run inference on the CPU/NPU.

## ğŸ“ Usage Guide

1.  **Onboarding**: Select your class (Grade) to personalize the AI.
2.  **Download Model**: Go to the **Host** tab and download a model (e.g., "Fast" for Qwen).
3.  **Chat**: Return to the **Chat** tab. The model will load automatically.
4.  **History**: Tap the **Time/Clock** icon in the top-left to see past chats.
5.  **Settings**: Customize theme and language in the **Settings** tab.

## ğŸ“„ License
[MIT](LICENSE)
